{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af6dcd8-4649-4d07-99bf-eab2946afa1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/03 19:21:33 WARN Utils: Your hostname, figo-Vostro-V131, resolves to a loopback address: 127.0.1.1; using 10.135.44.75 instead (on interface wlp9s0)\n",
      "25/12/03 19:21:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/03 19:21:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/12/03 19:21:49 WARN Instrumentation: [6602a648] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.9999999999999992]\n",
      "Intercept: 15.000000000000009\n"
     ]
    }
   ],
   "source": [
    "# Import library SparkSession untuk memulai sesi spark\n",
    "from pyspark.sql import SparkSession\n",
    "# Import algoritma LinearRegression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "# Import algoritma VectorAssembler untuk mengubah data menjadi vector\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Inisiasi sesi spark\n",
    "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
    "\n",
    "# Membuat sebuah dataset yang nantinya akan diprediksi data selanjutnya\n",
    "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
    "# Membuat nama - nama kolom dari dataset tersebut\n",
    "columns = ['ID', 'Feature', 'Target']\n",
    "# Mengubah dataset yang dibuat menjadi dataframe pyspark\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Mengubah kolom Feature menjadi vektor terlebih dahulu\n",
    "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "# Mempelajari data Fatures berdasarkan target dengan algoritma LinearRegression\n",
    "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
    "model = lr.fit(df_transformed)\n",
    "\n",
    "# Menampilkan koefisien dari data dan intercept dari data\n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d64f0398-533e-427a-9eb9-54cd14e762d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 19:44:57 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-12.262057941669692,4.087352270650961]\n",
      "Intercept: 11.568912739136307\n"
     ]
    }
   ],
   "source": [
    "# Import library spark untuk mengubah list menjadi vektor agar bisa di olah oleh Mllib PySPark\n",
    "from pyspark.ml.linalg import Vectors\n",
    "# Untuk memulai session\n",
    "from pyspark.sql import SparkSession\n",
    "# Melakukan klasifikasi denganl library LogisticRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Untuk memulai session spark\n",
    "spark = SparkSession.builder.appName('LogReg Example').getOrCreate()\n",
    "\n",
    "# Menyiapkan dataset dalam bentuk vektor\n",
    "data = [\n",
    "    (1, Vectors.dense([2.0, 3.0]), 0),\n",
    "    (2, Vectors.dense([1.0, 5.0]), 1),\n",
    "    (3, Vectors.dense([2.5, 4.5]), 1),\n",
    "    (4, Vectors.dense([3.0, 6.0]), 0)\n",
    "]\n",
    "# Menyiapkan nama - nama kolom dataset\n",
    "columns = ['ID', 'Features', 'Label']\n",
    "\n",
    "# Mengubah dataset tersebut menjadi dataframe PySpark\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Train logistic regression model\n",
    "lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n",
    "model = lr.fit(df)\n",
    "\n",
    "# Menampilkan koefisien dari model LogistikRegression\n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d07e4e15-0b14-4110-b7fb-1806a1f5930f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: [array([12.5, 12.5]), array([3., 3.])]\n"
     ]
    }
   ],
   "source": [
    "# Import library KMeans untuk pengelompokan data\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Menyiapkan sebuah dataset yang akan dikelompokan berdasarkan kemiripan data\n",
    "data = [\n",
    "    (1, Vectors.dense([1.0, 1.0])), \n",
    "    (2, Vectors.dense([5.0, 5.0])), \n",
    "    (3, Vectors.dense([10.0, 10.0])), \n",
    "    (4, Vectors.dense([15.0, 15.0]))]\n",
    "# Menyiapkan nama - nama kolom dari dataset\n",
    "columns = ['ID', 'Features']\n",
    "# Mengubah dataset tersebut menjadi dataframe\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Melatih mode KMEans\n",
    "kmeans = KMeans(featuresCol='Features', k=2)\n",
    "model = kmeans.fit(df)\n",
    "\n",
    "# menunjukan nilai tengah dari model\n",
    "centers = model.clusterCenters()\n",
    "print(f'Cluster Centers: {centers}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b71e927-b49d-413e-bc39-56f804371ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/03 21:55:53 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+----------+\n",
      "|            features|Survived|prediction|\n",
      "+--------------------+--------+----------+\n",
      "|[2.0,0.0,62.0,0.0...|       0|       0.0|\n",
      "|[3.0,1.0,30.0,0.0...|       1|       1.0|\n",
      "|[3.0,1.0,18.0,0.0...|       1|       1.0|\n",
      "|[2.0,0.0,63.0,1.0...|       0|       0.0|\n",
      "|[3.0,1.0,45.0,0.0...|       1|       1.0|\n",
      "+--------------------+--------+----------+\n",
      "only showing top 5 rows\n",
      "Test Accuracy = 1.0000\n",
      "\n",
      "Confusion Matrix:\n",
      "Predicted  0.0  1.0\n",
      "Actual             \n",
      "0           32    0\n",
      "1            0   24\n"
     ]
    }
   ],
   "source": [
    "# Import beberapa library yang dibutuhkan\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "import pandas as pd\n",
    "\n",
    "# Inisiasi sebuah sesi pyspark \n",
    "spark = SparkSession.builder.appName(\"Titanic Classification\").getOrCreate()\n",
    "\n",
    "# Menjadikan dataset titanic menjadi dataframe\n",
    "df = spark.read.csv(\"Titanic_clean.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# ISi semua kolom yang sama\n",
    "df = df.fillna({'Age': df.select('Age').agg({'Age':'mean'}).first()[0],\n",
    "                'Embarked': 'S'})\n",
    "\n",
    "# MElakukan sebuah proses encoding\n",
    "# Mendeklarasikan StringIndexr untuk mengencode kolom - kolom yang string\n",
    "sex_indexer = StringIndexer(inputCol='Sex', outputCol='SexIndex')\n",
    "embarked_indexer = StringIndexer(inputCol='Embarked', outputCol='EmbarkedIndex')\n",
    "df = sex_indexer.fit(df).transform(df)\n",
    "df = embarked_indexer.fit(df).transform(df)\n",
    "\n",
    "# Mengubah dataset vektor\n",
    "feature_cols = ['Pclass', 'SexIndex', 'Age', 'SibSp', 'Parch', 'Fare', 'EmbarkedIndex']\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Membagi data dari data traning dan spliting\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Mendeklarasikan model\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='Survived', maxIter=50)\n",
    "\n",
    "# TUnning model dengan hyperparameter\n",
    "\n",
    "# Pendeklarasian untuk mengukur tingkat keberhasilan model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='Survived', predictionCol='prediction', metricName='accuracy')\n",
    "# Buat hyper parameter\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .build()\n",
    "\n",
    "# Pendeklarasian sebuah model mulai dari nama algoritma, estimtaor\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=5)\n",
    "\n",
    "# 9. Train model\n",
    "cv_model = crossval.fit(train_df)\n",
    "\n",
    "# 10. Make predictions\n",
    "predictions = cv_model.transform(test_df)\n",
    "predictions.select(\"features\", \"Survived\", \"prediction\").show(5)\n",
    "\n",
    "# 11. Evaluate accuracy\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy = {accuracy:.4f}\")\n",
    "\n",
    "# 12. Confusion Matrix\n",
    "pred_labels = predictions.select(\"prediction\", \"Survived\").toPandas()\n",
    "confusion_matrix = pd.crosstab(pred_labels['Survived'], pred_labels['prediction'], rownames=['Actual'], colnames=['Predicted'])\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
